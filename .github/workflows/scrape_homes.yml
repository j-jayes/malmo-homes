name: Run Scrape Script for Properties

on:
  schedule:
    # set the cron job to every week on Monday at 00:30
    - cron: '30 0 * * 1'
  workflow_dispatch:  # This line allows you to manually trigger the workflow from the GitHub web interface.

jobs:
  build:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0  # Ensures the entire repo history is checked out

    - name: Install system dependencies # Added in because of lxml failing
      run: |
        sudo apt-get update
        sudo apt install -y libxml2-dev libxslt-dev

    - name: Install Google Chrome
      run: |
        sudo apt update
        sudo apt install -y google-chrome-stable

    - name: Install ChromeDriver
      run: |
        sudo apt install -y chromium-chromedriver
        sudo ln -s /usr/lib/chromium-browser/chromedriver /usr/bin/chromedriver


    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.12'  # Adjust Python version as necessary for your project

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Set PYTHONPATH
      run: echo "PYTHONPATH=${{ github.workspace }}" >> $GITHUB_ENV
  
    - name: Run script
      run: python src/011-scrape-properties-gh-actions.py

    - name: Commit changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add .
        git commit -m "Update data from scrape script for properties" -a || echo "No changes to commit"
        git push
