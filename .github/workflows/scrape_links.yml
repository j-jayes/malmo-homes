name: Run Scrape Script for Links

on:
  schedule:
    # set the cron job to every week on Sunday at 00:00
    - cron: '0 0 * * 0'
  workflow_dispatch:  # This line allows you to manually trigger the workflow from the GitHub web interface.

jobs:
  build:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0  # Ensures the entire repo history is checked out

    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.x'  # Adjust Python version as necessary for your project

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Set PYTHONPATH
      run: echo "PYTHONPATH=${{ github.workspace }}" >> $GITHUB_ENV
  
    - name: Run script
      run: python src/011-scrape-links-gh-actions.py

    - name: Commit changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        git add .
        git commit -m "Update data from scrape script" -a || echo "No changes to commit"
        git push
